-- Automatically generated by SQLQueryTestSuite
-- !query
SET hivevar:colname = 'c'
-- !query analysis
SetCommand (hivevar:colname,Some('c'))


-- !query
SELECT IDENTIFIER(${colname} || '_1') FROM VALUES(1) AS T(c_1)
-- !query analysis
Project [c_1#x]
+- SubqueryAlias T
   +- LocalRelation [c_1#x]


-- !query
SELECT IDENTIFIER('c1') FROM VALUES(1) AS T(c1)
-- !query analysis
Project [c1#x]
+- SubqueryAlias T
   +- LocalRelation [c1#x]


-- !query
SELECT IDENTIFIER('t.c1') FROM VALUES(1) AS T(c1)
-- !query analysis
Project [c1#x]
+- SubqueryAlias T
   +- LocalRelation [c1#x]


-- !query
SELECT IDENTIFIER('`t`.c1') FROM VALUES(1) AS T(c1)
-- !query analysis
Project [c1#x]
+- SubqueryAlias T
   +- LocalRelation [c1#x]


-- !query
SELECT IDENTIFIER('`c 1`') FROM VALUES(1) AS T(`c 1`)
-- !query analysis
Project [c 1#x]
+- SubqueryAlias T
   +- LocalRelation [c 1#x]


-- !query
SELECT IDENTIFIER('``') FROM VALUES(1) AS T(``)
-- !query analysis
Project [#x]
+- SubqueryAlias T
   +- LocalRelation [#x]


-- !query
SELECT IDENTIFIER('c' || '1') FROM VALUES(1) AS T(c1)
-- !query analysis
Project [c1#x]
+- SubqueryAlias T
   +- LocalRelation [c1#x]


-- !query
CREATE SCHEMA IF NOT EXISTS s
-- !query analysis
CreateNamespace true
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [s]


-- !query
CREATE TABLE s.tab(c1 INT) USING CSV
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`s`.`tab`, false


-- !query
USE SCHEMA s
-- !query analysis
SetNamespaceCommand [s]


-- !query
INSERT INTO IDENTIFIER('ta' || 'b') VALUES(1)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/s.db/tab, false, CSV, [path=file:[not included in comparison]/{warehouse_dir}/s.db/tab], Append, `spark_catalog`.`s`.`tab`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/s.db/tab), [c1]
+- Project [col1#x AS c1#x]
   +- LocalRelation [col1#x]


-- !query
DELETE FROM IDENTIFIER('ta' || 'b') WHERE 1=0
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNSUPPORTED_FEATURE.TABLE_OPERATION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "operation" : "DELETE",
    "tableName" : "`spark_catalog`.`s`.`tab`"
  }
}


-- !query
UPDATE IDENTIFIER('ta' || 'b') SET c1 = 2
-- !query analysis
org.apache.spark.SparkUnsupportedOperationException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_2096",
  "messageParameters" : {
    "ddl" : "UPDATE TABLE"
  }
}


-- !query
MERGE INTO IDENTIFIER('ta' || 'b') AS t USING IDENTIFIER('ta' || 'b') AS s ON s.c1 = t.c1
  WHEN MATCHED THEN UPDATE SET c1 = 3
-- !query analysis
org.apache.spark.SparkUnsupportedOperationException
{
  "errorClass" : "_LEGACY_ERROR_TEMP_2096",
  "messageParameters" : {
    "ddl" : "MERGE INTO TABLE"
  }
}


-- !query
SELECT * FROM IDENTIFIER('tab')
-- !query analysis
Project [c1#x]
+- SubqueryAlias spark_catalog.s.tab
   +- Relation spark_catalog.s.tab[c1#x] csv


-- !query
SELECT * FROM IDENTIFIER('s.tab')
-- !query analysis
Project [c1#x]
+- SubqueryAlias spark_catalog.s.tab
   +- Relation spark_catalog.s.tab[c1#x] csv


-- !query
SELECT * FROM IDENTIFIER('`s`.`tab`')
-- !query analysis
Project [c1#x]
+- SubqueryAlias spark_catalog.s.tab
   +- Relation spark_catalog.s.tab[c1#x] csv


-- !query
SELECT * FROM IDENTIFIER('t' || 'a' || 'b')
-- !query analysis
Project [c1#x]
+- SubqueryAlias spark_catalog.s.tab
   +- Relation spark_catalog.s.tab[c1#x] csv


-- !query
USE SCHEMA default
-- !query analysis
SetNamespaceCommand [default]


-- !query
DROP TABLE s.tab
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), s.tab


-- !query
DROP SCHEMA s
-- !query analysis
DropNamespace false, false
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [s]


-- !query
SELECT IDENTIFIER('COAL' || 'ESCE')(NULL, 1)
-- !query analysis
Project [coalesce(cast(null as int), 1) AS coalesce(NULL, 1)#x]
+- OneRowRelation


-- !query
SELECT IDENTIFIER('abs')(c1) FROM VALUES(-1) AS T(c1)
-- !query analysis
Project [abs(c1#x) AS abs(c1)#x]
+- SubqueryAlias T
   +- LocalRelation [c1#x]


-- !query
SELECT * FROM IDENTIFIER('ra' || 'nge')(0, 1)
-- !query analysis
Project [id#xL]
+- Range (0, 1, step=1)


-- !query
CREATE TABLE IDENTIFIER('tab')(c1 INT) USING CSV
-- !query analysis
org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException
{
  "errorClass" : "TABLE_OR_VIEW_ALREADY_EXISTS",
  "sqlState" : "42P07",
  "messageParameters" : {
    "relationName" : "`spark_catalog`.`default`.`tab`"
  }
}


-- !query
DROP TABLE IF EXISTS IDENTIFIER('ta' || 'b')
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.tab


-- !query
CREATE SCHEMA identifier_clauses
-- !query analysis
CreateNamespace false
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [identifier_clauses]


-- !query
USE identifier_clauses
-- !query analysis
SetCatalogAndNamespace
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [identifier_clauses]


-- !query
CREATE TABLE IDENTIFIER('ta' || 'b')(c1 INT) USING CSV
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`identifier_clauses`.`tab`, false


-- !query
DROP TABLE IF EXISTS IDENTIFIER('identifier_clauses.' || 'tab')
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), identifier_clauses.tab


-- !query
CREATE TABLE IDENTIFIER('identifier_clauses.' || 'tab')(c1 INT) USING CSV
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`identifier_clauses`.`tab`, false


-- !query
REPLACE TABLE IDENTIFIER('identifier_clauses.' || 'tab')(c1 INT) USING CSV
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNSUPPORTED_FEATURE.TABLE_OPERATION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "operation" : "REPLACE TABLE",
    "tableName" : "`spark_catalog`.`identifier_clauses`.`tab`"
  }
}


-- !query
CACHE TABLE IDENTIFIER('ta' || 'b')
-- !query analysis
CacheTable [tab], false, true
   +- SubqueryAlias spark_catalog.identifier_clauses.tab
      +- Relation spark_catalog.identifier_clauses.tab[c1#x] csv


-- !query
UNCACHE TABLE IDENTIFIER('ta' || 'b')
-- !query analysis
UncacheTable false, true
   +- SubqueryAlias spark_catalog.identifier_clauses.tab
      +- Relation spark_catalog.identifier_clauses.tab[c1#x] csv


-- !query
DROP TABLE IF EXISTS IDENTIFIER('ta' || 'b')
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), identifier_clauses.tab


-- !query
USE default
-- !query analysis
SetCatalogAndNamespace
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [default]


-- !query
DROP SCHEMA identifier_clauses
-- !query analysis
DropNamespace false, false
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [identifier_clauses]


-- !query
CREATE TABLE tab(c1 INT) USING CSV
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`tab`, false


-- !query
INSERT INTO tab VALUES (1)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/tab, false, CSV, [path=file:[not included in comparison]/{warehouse_dir}/tab], Append, `spark_catalog`.`default`.`tab`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/tab), [c1]
+- Project [col1#x AS c1#x]
   +- LocalRelation [col1#x]


-- !query
SELECT c1 FROM tab
-- !query analysis
Project [c1#x]
+- SubqueryAlias spark_catalog.default.tab
   +- Relation spark_catalog.default.tab[c1#x] csv


-- !query
DESCRIBE IDENTIFIER('ta' || 'b')
-- !query analysis
DescribeTableCommand `spark_catalog`.`default`.`tab`, false, [col_name#x, data_type#x, comment#x]


-- !query
ANALYZE TABLE IDENTIFIER('ta' || 'b') COMPUTE STATISTICS
-- !query analysis
AnalyzeTableCommand `spark_catalog`.`default`.`tab`, false


-- !query
ALTER TABLE IDENTIFIER('ta' || 'b') ADD COLUMN c2 INT
-- !query analysis
AlterTableAddColumnsCommand `spark_catalog`.`default`.`tab`, [StructField(c2,IntegerType,true)]


-- !query
SHOW TBLPROPERTIES IDENTIFIER('ta' || 'b')
-- !query analysis
ShowTableProperties [key#x, value#x]
+- ResolvedTable V2SessionCatalog(spark_catalog), default.tab, V1Table(default.tab), [c1#x, c2#x]


-- !query
SHOW COLUMNS FROM IDENTIFIER('ta' || 'b')
-- !query analysis
ShowColumnsCommand `spark_catalog`.`default`.`tab`, [col_name#x]


-- !query
COMMENT ON TABLE IDENTIFIER('ta' || 'b') IS 'hello'
-- !query analysis
CommentOnTable hello
+- ResolvedTable V2SessionCatalog(spark_catalog), default.tab, V1Table(default.tab), [c1#x, c2#x]


-- !query
REFRESH TABLE IDENTIFIER('ta' || 'b')
-- !query analysis
RefreshTableCommand `spark_catalog`.`default`.`tab`


-- !query
REPAIR TABLE IDENTIFIER('ta' || 'b')
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "NOT_A_PARTITIONED_TABLE",
  "sqlState" : "42809",
  "messageParameters" : {
    "operation" : "MSCK REPAIR TABLE",
    "tableIdentWithDB" : "`spark_catalog`.`default`.`tab`"
  }
}


-- !query
TRUNCATE TABLE IDENTIFIER('ta' || 'b')
-- !query analysis
TruncateTableCommand `spark_catalog`.`default`.`tab`


-- !query
DROP TABLE IF EXISTS tab
-- !query analysis
DropTable true, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.tab


-- !query
CREATE OR REPLACE VIEW IDENTIFIER('v')(c1) AS VALUES(1)
-- !query analysis
CreateViewCommand `spark_catalog`.`default`.`v`, [(c1,None)], VALUES(1), false, true, PersistedView, COMPENSATION, true
   +- LocalRelation [col1#x]


-- !query
SELECT * FROM v
-- !query analysis
Project [c1#x]
+- SubqueryAlias spark_catalog.default.v
   +- View (`spark_catalog`.`default`.`v`, [c1#x])
      +- Project [cast(col1#x as int) AS c1#x]
         +- LocalRelation [col1#x]


-- !query
ALTER VIEW IDENTIFIER('v') AS VALUES(2)
-- !query analysis
AlterViewAsCommand `spark_catalog`.`default`.`v`, VALUES(2), true
   +- LocalRelation [col1#x]


-- !query
DROP VIEW IDENTIFIER('v')
-- !query analysis
DropTableCommand `spark_catalog`.`default`.`v`, false, true, false


-- !query
CREATE TEMPORARY VIEW IDENTIFIER('v')(c1) AS VALUES(1)
-- !query analysis
CreateViewCommand `v`, [(c1,None)], VALUES(1), false, false, LocalTempView, UNSUPPORTED, true
   +- LocalRelation [col1#x]


-- !query
DROP VIEW IDENTIFIER('v')
-- !query analysis
DropTempViewCommand v


-- !query
CREATE SCHEMA IDENTIFIER('id' || 'ent')
-- !query analysis
CreateNamespace false
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [ident]


-- !query
ALTER SCHEMA IDENTIFIER('id' || 'ent') SET PROPERTIES (somekey = 'somevalue')
-- !query analysis
SetNamespaceProperties [somekey=somevalue]
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [ident]


-- !query
ALTER SCHEMA IDENTIFIER('id' || 'ent') SET LOCATION 'someloc'
-- !query analysis
SetNamespaceLocation someloc
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [ident]


-- !query
COMMENT ON SCHEMA IDENTIFIER('id' || 'ent') IS 'some comment'
-- !query analysis
CommentOnNamespace some comment
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [ident]


-- !query
DESCRIBE SCHEMA IDENTIFIER('id' || 'ent')
-- !query analysis
DescribeNamespace false, [info_name#x, info_value#x]
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [ident]


-- !query
SHOW TABLES IN IDENTIFIER('id' || 'ent')
-- !query analysis
ShowTables [namespace#x, tableName#x, isTemporary#x]
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [ident]


-- !query
SHOW TABLE EXTENDED IN IDENTIFIER('id' || 'ent') LIKE 'hello'
-- !query analysis
ShowTablesCommand ident, hello, [namespace#x, tableName#x, isTemporary#x, information#x], true


-- !query
USE IDENTIFIER('id' || 'ent')
-- !query analysis
SetCatalogAndNamespace
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [ident]


-- !query
SHOW CURRENT SCHEMA
-- !query analysis
ShowCurrentNamespaceCommand


-- !query
USE SCHEMA IDENTIFIER('id' || 'ent')
-- !query analysis
SetNamespaceCommand [ident]


-- !query
USE SCHEMA default
-- !query analysis
SetNamespaceCommand [default]


-- !query
DROP SCHEMA IDENTIFIER('id' || 'ent')
-- !query analysis
DropNamespace false, false
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [ident]


-- !query
CREATE SCHEMA ident
-- !query analysis
CreateNamespace false
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [ident]


-- !query
CREATE FUNCTION IDENTIFIER('ident.' || 'myDoubleAvg') AS 'test.org.apache.spark.sql.MyDoubleAvg'
-- !query analysis
CreateFunctionCommand spark_catalog.ident.myDoubleAvg, test.org.apache.spark.sql.MyDoubleAvg, false, false, false


-- !query
DESCRIBE FUNCTION IDENTIFIER('ident.' || 'myDoubleAvg')
-- !query analysis
DescribeFunctionCommand org.apache.spark.sql.catalyst.expressions.ExpressionInfo@xxxxxxxx, false


-- !query
REFRESH FUNCTION IDENTIFIER('ident.' || 'myDoubleAvg')
-- !query analysis
RefreshFunctionCommand ident, mydoubleavg


-- !query
DROP FUNCTION IDENTIFIER('ident.' || 'myDoubleAvg')
-- !query analysis
DropFunctionCommand spark_catalog.ident.mydoubleavg, false, false


-- !query
DROP SCHEMA ident
-- !query analysis
DropNamespace false, false
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [ident]


-- !query
CREATE TEMPORARY FUNCTION IDENTIFIER('my' || 'DoubleAvg') AS 'test.org.apache.spark.sql.MyDoubleAvg'
-- !query analysis
CreateFunctionCommand myDoubleAvg, test.org.apache.spark.sql.MyDoubleAvg, true, false, false


-- !query
DROP TEMPORARY FUNCTION IDENTIFIER('my' || 'DoubleAvg')
-- !query analysis
DropFunctionCommand myDoubleAvg, false, true


-- !query
DECLARE var = 'sometable'
-- !query analysis
CreateVariable defaultvalueexpression(sometable, 'sometable'), false
+- ResolvedIdentifier org.apache.spark.sql.catalyst.analysis.FakeSystemCatalog$@xxxxxxxx, session.var


-- !query
CREATE TABLE IDENTIFIER(var)(c1 INT) USING CSV
-- !query analysis
org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException
{
  "errorClass" : "TABLE_OR_VIEW_ALREADY_EXISTS",
  "sqlState" : "42P07",
  "messageParameters" : {
    "relationName" : "`spark_catalog`.`default`.`sometable`"
  }
}


-- !query
SET VAR var = 'c1'
-- !query analysis
SetVariable [variablereference(system.session.var='sometable')]
+- Project [c1 AS var#x]
   +- OneRowRelation


-- !query
SELECT IDENTIFIER(var) FROM VALUES(1) AS T(c1)
-- !query analysis
Project [c1#x]
+- SubqueryAlias T
   +- LocalRelation [c1#x]


-- !query
SET VAR var = 'some'
-- !query analysis
SetVariable [variablereference(system.session.var='c1')]
+- Project [some AS var#x]
   +- OneRowRelation


-- !query
DROP TABLE IDENTIFIER(var || 'table')
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'var'",
    "hint" : ""
  }
}


-- !query
SELECT IDENTIFIER('c 1') FROM VALUES(1) AS T(`c 1`)
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_SYNTAX_ERROR",
  "sqlState" : "42601",
  "messageParameters" : {
    "error" : "'1'",
    "hint" : ": extra input '1'"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 24,
    "fragment" : "IDENTIFIER('c 1')"
  } ]
}


-- !query
SELECT IDENTIFIER('') FROM VALUES(1) AS T(``)
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "PARSE_EMPTY_STATEMENT",
  "sqlState" : "42617",
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 21,
    "fragment" : "IDENTIFIER('')"
  } ]
}


-- !query
VALUES(IDENTIFIER(CAST(NULL AS STRING)))
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "NOT_A_CONSTANT_STRING.NULL",
  "sqlState" : "42601",
  "messageParameters" : {
    "expr" : "CAST(NULL AS STRING)",
    "name" : "IDENTIFIER"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 19,
    "stopIndex" : 38,
    "fragment" : "CAST(NULL AS STRING)"
  } ]
}


-- !query
VALUES(IDENTIFIER(1))
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "NOT_A_CONSTANT_STRING.WRONG_TYPE",
  "sqlState" : "42601",
  "messageParameters" : {
    "dataType" : "int",
    "expr" : "1",
    "name" : "IDENTIFIER"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 19,
    "stopIndex" : 19,
    "fragment" : "1"
  } ]
}


-- !query
VALUES(IDENTIFIER(SUBSTR('HELLO', 1, RAND() + 1)))
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "NOT_A_CONSTANT_STRING.NOT_CONSTANT",
  "sqlState" : "42601",
  "messageParameters" : {
    "expr" : "substr('HELLO', 1, CAST((rand() + CAST(1 AS DOUBLE)) AS INT))",
    "name" : "IDENTIFIER"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 19,
    "stopIndex" : 48,
    "fragment" : "SUBSTR('HELLO', 1, RAND() + 1)"
  } ]
}


-- !query
SELECT `IDENTIFIER`('abs')(c1) FROM VALUES(-1) AS T(c1)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNRESOLVED_ROUTINE",
  "sqlState" : "42883",
  "messageParameters" : {
    "routineName" : "`IDENTIFIER`",
    "searchPath" : "[`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`]"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 26,
    "fragment" : "`IDENTIFIER`('abs')"
  } ]
}


-- !query
CREATE TABLE IDENTIFIER(1)(c1 INT) USING csv
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "NOT_A_CONSTANT_STRING.WRONG_TYPE",
  "sqlState" : "42601",
  "messageParameters" : {
    "dataType" : "int",
    "expr" : "1",
    "name" : "IDENTIFIER"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 25,
    "stopIndex" : 25,
    "fragment" : "1"
  } ]
}


-- !query
CREATE TABLE IDENTIFIER('a.b.c')(c1 INT) USING csv
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "REQUIRES_SINGLE_PART_NAMESPACE",
  "sqlState" : "42K05",
  "messageParameters" : {
    "namespace" : "`a`.`b`",
    "sessionCatalog" : "spark_catalog"
  }
}


-- !query
CREATE VIEW IDENTIFIER('a.b.c')(c1) AS VALUES(1)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "REQUIRES_SINGLE_PART_NAMESPACE",
  "sqlState" : "42K05",
  "messageParameters" : {
    "namespace" : "`a`.`b`",
    "sessionCatalog" : "spark_catalog"
  }
}


-- !query
DROP TABLE IDENTIFIER('a.b.c')
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "REQUIRES_SINGLE_PART_NAMESPACE",
  "sqlState" : "42K05",
  "messageParameters" : {
    "namespace" : "`a`.`b`",
    "sessionCatalog" : "spark_catalog"
  }
}


-- !query
DROP VIEW IDENTIFIER('a.b.c')
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "REQUIRES_SINGLE_PART_NAMESPACE",
  "sqlState" : "42K05",
  "messageParameters" : {
    "namespace" : "`a`.`b`",
    "sessionCatalog" : "spark_catalog"
  }
}


-- !query
COMMENT ON TABLE IDENTIFIER('a.b.c.d') IS 'hello'
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "REQUIRES_SINGLE_PART_NAMESPACE",
  "sqlState" : "42K05",
  "messageParameters" : {
    "namespace" : "`a`.`b`.`c`",
    "sessionCatalog" : "spark_catalog"
  }
}


-- !query
VALUES(IDENTIFIER(1)())
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "NOT_A_CONSTANT_STRING.WRONG_TYPE",
  "sqlState" : "42601",
  "messageParameters" : {
    "dataType" : "int",
    "expr" : "1",
    "name" : "IDENTIFIER"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 19,
    "stopIndex" : 19,
    "fragment" : "1"
  } ]
}


-- !query
VALUES(IDENTIFIER('a.b.c.d')())
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "IDENTIFIER_TOO_MANY_NAME_PARTS",
  "sqlState" : "42601",
  "messageParameters" : {
    "identifier" : "`a`.`b`.`c`.`d`",
    "limit" : "2"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 30,
    "fragment" : "IDENTIFIER('a.b.c.d')()"
  } ]
}


-- !query
CREATE TEMPORARY FUNCTION IDENTIFIER('default.my' || 'DoubleAvg') AS 'test.org.apache.spark.sql.MyDoubleAvg'
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SQL_SYNTAX.CREATE_TEMP_FUNC_WITH_DATABASE",
  "sqlState" : "42000",
  "messageParameters" : {
    "database" : "`default`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 108,
    "fragment" : "CREATE TEMPORARY FUNCTION IDENTIFIER('default.my' || 'DoubleAvg') AS 'test.org.apache.spark.sql.MyDoubleAvg'"
  } ]
}


-- !query
DROP TEMPORARY FUNCTION IDENTIFIER('default.my' || 'DoubleAvg')
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "INVALID_SQL_SYNTAX.MULTI_PART_NAME",
  "sqlState" : "42000",
  "messageParameters" : {
    "name" : "`default`.`myDoubleAvg`",
    "statement" : "DROP TEMPORARY FUNCTION"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 63,
    "fragment" : "DROP TEMPORARY FUNCTION IDENTIFIER('default.my' || 'DoubleAvg')"
  } ]
}


-- !query
CREATE TEMPORARY VIEW IDENTIFIER('default.v')(c1) AS VALUES(1)
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "TEMP_VIEW_NAME_TOO_MANY_NAME_PARTS",
  "sqlState" : "428EK",
  "messageParameters" : {
    "actualName" : "`default`.`v`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 1,
    "stopIndex" : 62,
    "fragment" : "CREATE TEMPORARY VIEW IDENTIFIER('default.v')(c1) AS VALUES(1)"
  } ]
}


-- !query
create temporary view identifier('v1') as (select my_col from (values (1), (2), (1) as (my_col)) group by 1)
-- !query analysis
CreateViewCommand `v1`, (select my_col from (values (1), (2), (1) as (my_col)) group by 1), false, false, LocalTempView, UNSUPPORTED, true
   +- Aggregate [my_col#x], [my_col#x]
      +- SubqueryAlias __auto_generated_subquery_name
         +- SubqueryAlias as
            +- LocalRelation [my_col#x]


-- !query
cache table identifier('t1') as (select my_col from (values (1), (2), (1) as (my_col)) group by 1)
-- !query analysis
CacheTableAsSelect t1, (select my_col from (values (1), (2), (1) as (my_col)) group by 1), false, true
   +- Aggregate [my_col#x], [my_col#x]
      +- SubqueryAlias __auto_generated_subquery_name
         +- SubqueryAlias as
            +- LocalRelation [my_col#x]


-- !query
create table identifier('t2') using csv as (select my_col from (values (1), (2), (1) as (my_col)) group by 1)
-- !query analysis
CreateDataSourceTableAsSelectCommand `spark_catalog`.`default`.`t2`, ErrorIfExists, [my_col]
   +- Aggregate [my_col#x], [my_col#x]
      +- SubqueryAlias __auto_generated_subquery_name
         +- SubqueryAlias as
            +- LocalRelation [my_col#x]


-- !query
insert into identifier('t2') select my_col from (values (3) as (my_col)) group by 1
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t2, false, CSV, [path=file:[not included in comparison]/{warehouse_dir}/t2], Append, `spark_catalog`.`default`.`t2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t2), [my_col]
+- Project [my_col#x AS my_col#x]
   +- Aggregate [my_col#x], [my_col#x]
      +- SubqueryAlias __auto_generated_subquery_name
         +- SubqueryAlias as
            +- LocalRelation [my_col#x]


-- !query
drop view v1
-- !query analysis
DropTempViewCommand v1


-- !query
drop table t1
-- !query analysis
DropTempViewCommand t1


-- !query
drop table t2
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.t2


-- !query
DECLARE agg = 'max'
-- !query analysis
CreateVariable defaultvalueexpression(max, 'max'), false
+- ResolvedIdentifier org.apache.spark.sql.catalyst.analysis.FakeSystemCatalog$@xxxxxxxx, session.agg


-- !query
DECLARE col = 'c1'
-- !query analysis
CreateVariable defaultvalueexpression(c1, 'c1'), false
+- ResolvedIdentifier org.apache.spark.sql.catalyst.analysis.FakeSystemCatalog$@xxxxxxxx, session.col


-- !query
DECLARE tab = 'T'
-- !query analysis
CreateVariable defaultvalueexpression(T, 'T'), false
+- ResolvedIdentifier org.apache.spark.sql.catalyst.analysis.FakeSystemCatalog$@xxxxxxxx, session.tab


-- !query
WITH S(c1, c2) AS (VALUES(1, 2), (2, 3)),
     T(c1, c2) AS (VALUES ('a', 'b'), ('c', 'd'))
SELECT IDENTIFIER(agg)(IDENTIFIER(col)) FROM IDENTIFIER(tab)
-- !query analysis
WithCTE
:- CTERelationDef xxxx, false
:  +- SubqueryAlias S
:     +- Project [col1#x AS c1#x, col2#x AS c2#x]
:        +- LocalRelation [col1#x, col2#x]
:- CTERelationDef xxxx, false
:  +- SubqueryAlias T
:     +- Project [col1#x AS c1#x, col2#x AS c2#x]
:        +- LocalRelation [col1#x, col2#x]
+- Aggregate [max(c1#x) AS max(c1)#x]
   +- SubqueryAlias T
      +- CTERelationRef xxxx, true, [c1#x, c2#x], false, false, 2


-- !query
WITH S(c1, c2) AS (VALUES(1, 2), (2, 3)),
     T(c1, c2) AS (VALUES ('a', 'b'), ('c', 'd'))
SELECT IDENTIFIER('max')(IDENTIFIER('c1')) FROM IDENTIFIER('T')
-- !query analysis
WithCTE
:- CTERelationDef xxxx, false
:  +- SubqueryAlias S
:     +- Project [col1#x AS c1#x, col2#x AS c2#x]
:        +- LocalRelation [col1#x, col2#x]
:- CTERelationDef xxxx, false
:  +- SubqueryAlias T
:     +- Project [col1#x AS c1#x, col2#x AS c2#x]
:        +- LocalRelation [col1#x, col2#x]
+- Aggregate [max(c1#x) AS max(c1)#x]
   +- SubqueryAlias T
      +- CTERelationRef xxxx, true, [c1#x, c2#x], false, false, 2


-- !query
WITH ABC(c1, c2) AS (VALUES(1, 2), (2, 3))
SELECT IDENTIFIER('max')(IDENTIFIER('c1')) FROM IDENTIFIER('A' || 'BC')
-- !query analysis
WithCTE
:- CTERelationDef xxxx, false
:  +- SubqueryAlias ABC
:     +- Project [col1#x AS c1#x, col2#x AS c2#x]
:        +- LocalRelation [col1#x, col2#x]
+- Aggregate [max(c1#x) AS max(c1)#x]
   +- SubqueryAlias ABC
      +- CTERelationRef xxxx, true, [c1#x, c2#x], false, false, 2


-- !query
CREATE TABLE IDENTIFIER('id_lite_col_test')(IDENTIFIER('col1') INT, IDENTIFIER('col2') STRING) USING CSV
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`id_lite_col_test`, false


-- !query
INSERT INTO IDENTIFIER('id_lite_col_test') VALUES (1, 'test')
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/id_lite_col_test, false, CSV, [path=file:[not included in comparison]/{warehouse_dir}/id_lite_col_test], Append, `spark_catalog`.`default`.`id_lite_col_test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/id_lite_col_test), [col1, col2]
+- Project [col1#x AS col1#x, col2#x AS col2#x]
   +- LocalRelation [col1#x, col2#x]


-- !query
SELECT IDENTIFIER('col1'), IDENTIFIER('col2') FROM IDENTIFIER('id_lite_col_test')
-- !query analysis
Project [col1#x, col2#x]
+- SubqueryAlias spark_catalog.default.id_lite_col_test
   +- Relation spark_catalog.default.id_lite_col_test[col1#x,col2#x] csv


-- !query
DROP TABLE IDENTIFIER('id_lite_col_test')
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.id_lite_col_test


-- !query
CREATE TABLE IDENTIFIER('id_lite_alter')(c1 INT) USING CSV
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`id_lite_alter`, false


-- !query
ALTER TABLE IDENTIFIER('id_lite_alter') RENAME COLUMN IDENTIFIER('c1') TO IDENTIFIER('col1')
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNSUPPORTED_FEATURE.TABLE_OPERATION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "operation" : "RENAME COLUMN",
    "tableName" : "`spark_catalog`.`default`.`id_lite_alter`"
  }
}


-- !query
ALTER TABLE IDENTIFIER('id_lite_alter') ADD COLUMN IDENTIFIER('c2') INT
-- !query analysis
AlterTableAddColumnsCommand `spark_catalog`.`default`.`id_lite_alter`, [StructField(c2,IntegerType,true)]


-- !query
ALTER TABLE IDENTIFIER('id_lite_alter') DROP COLUMN IDENTIFIER('c2')
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "UNSUPPORTED_FEATURE.TABLE_OPERATION",
  "sqlState" : "0A000",
  "messageParameters" : {
    "operation" : "DROP COLUMN",
    "tableName" : "`spark_catalog`.`default`.`id_lite_alter`"
  }
}


-- !query
ALTER TABLE IDENTIFIER('id_lite_alter') RENAME TO IDENTIFIER('id_lite_renamed')
-- !query analysis
AlterTableRenameCommand `spark_catalog`.`default`.`id_lite_alter`, `id_lite_renamed`, false


-- !query
DROP TABLE IDENTIFIER('id_lite_renamed')
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.id_lite_renamed


-- !query
CREATE SCHEMA identifier_lite_schema
-- !query analysis
CreateNamespace false
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [identifier_lite_schema]


-- !query
CREATE TABLE IDENTIFIER('identifier_lite_schema.qualified_test')(c1 INT) USING CSV
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`identifier_lite_schema`.`qualified_test`, false


-- !query
INSERT INTO IDENTIFIER('identifier_lite_schema.qualified_test') VALUES(42)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/identifier_lite_schema.db/qualified_test, false, CSV, [path=file:[not included in comparison]/{warehouse_dir}/identifier_lite_schema.db/qualified_test], Append, `spark_catalog`.`identifier_lite_schema`.`qualified_test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/identifier_lite_schema.db/qualified_test), [c1]
+- Project [col1#x AS c1#x]
   +- LocalRelation [col1#x]


-- !query
SELECT * FROM IDENTIFIER('identifier_lite_schema.qualified_test')
-- !query analysis
Project [c1#x]
+- SubqueryAlias spark_catalog.identifier_lite_schema.qualified_test
   +- Relation spark_catalog.identifier_lite_schema.qualified_test[c1#x] csv


-- !query
DROP TABLE IDENTIFIER('identifier_lite_schema.qualified_test')
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), identifier_lite_schema.qualified_test


-- !query
DROP SCHEMA identifier_lite_schema
-- !query analysis
DropNamespace false, false
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [identifier_lite_schema]


-- !query
CREATE SCHEMA cat1
-- !query analysis
CreateNamespace false
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [cat1]


-- !query
CREATE TABLE cat1.tab1(c1 INT) USING CSV
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`cat1`.`tab1`, false


-- !query
INSERT INTO IDENTIFIER('cat1').tab1 VALUES(1)
-- !query analysis
InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/cat1.db/tab1, false, CSV, [path=file:[not included in comparison]/{warehouse_dir}/cat1.db/tab1], Append, `spark_catalog`.`cat1`.`tab1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/cat1.db/tab1), [c1]
+- Project [col1#x AS c1#x]
   +- LocalRelation [col1#x]


-- !query
SELECT * FROM IDENTIFIER('cat1').tab1
-- !query analysis
Project [c1#x]
+- SubqueryAlias spark_catalog.cat1.tab1
   +- Relation spark_catalog.cat1.tab1[c1#x] csv


-- !query
SELECT * FROM IDENTIFIER('cat1.tab1')
-- !query analysis
Project [c1#x]
+- SubqueryAlias spark_catalog.cat1.tab1
   +- Relation spark_catalog.cat1.tab1[c1#x] csv


-- !query
SELECT * FROM IDENTIFIER('cat1').IDENTIFIER('tab1')
-- !query analysis
Project [c1#x]
+- SubqueryAlias spark_catalog.cat1.tab1
   +- Relation spark_catalog.cat1.tab1[c1#x] csv


-- !query
DROP TABLE cat1.tab1
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), cat1.tab1


-- !query
DROP SCHEMA cat1
-- !query analysis
DropNamespace false, false
+- ResolvedNamespace V2SessionCatalog(spark_catalog), [cat1]


-- !query
CREATE SCHEMA `schema 1`
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "INVALID_SCHEMA_OR_RELATION_NAME",
  "sqlState" : "42602",
  "messageParameters" : {
    "name" : "`schema 1`"
  }
}


-- !query
CREATE TABLE `schema 1`.`table 1`(c1 INT) USING CSV
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "INVALID_SCHEMA_OR_RELATION_NAME",
  "sqlState" : "42602",
  "messageParameters" : {
    "name" : "`table 1`"
  }
}


-- !query
INSERT INTO IDENTIFIER('`schema 1`.`table 1`') VALUES(100)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "TABLE_OR_VIEW_NOT_FOUND",
  "sqlState" : "42P01",
  "messageParameters" : {
    "relationName" : "`schema 1`.`table 1`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 13,
    "stopIndex" : 46,
    "fragment" : "IDENTIFIER('`schema 1`.`table 1`')"
  } ]
}


-- !query
SELECT * FROM IDENTIFIER('`schema 1`.`table 1`')
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "TABLE_OR_VIEW_NOT_FOUND",
  "sqlState" : "42P01",
  "messageParameters" : {
    "relationName" : "`schema 1`.`table 1`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 48,
    "fragment" : "IDENTIFIER('`schema 1`.`table 1`')"
  } ]
}


-- !query
SELECT * FROM IDENTIFIER('`schema 1`').`table 1`
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "TABLE_OR_VIEW_NOT_FOUND",
  "sqlState" : "42P01",
  "messageParameters" : {
    "relationName" : "`schema 1`.`table 1`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 48,
    "fragment" : "IDENTIFIER('`schema 1`').`table 1`"
  } ]
}


-- !query
DROP TABLE `schema 1`.`table 1`
-- !query analysis
org.apache.spark.sql.catalyst.analysis.NoSuchTableException
{
  "errorClass" : "TABLE_OR_VIEW_NOT_FOUND",
  "sqlState" : "42P01",
  "messageParameters" : {
    "relationName" : "`spark_catalog`.`schema 1`.`table 1`"
  }
}


-- !query
DROP SCHEMA `schema 1`
-- !query analysis
org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException
{
  "errorClass" : "SCHEMA_NOT_FOUND",
  "sqlState" : "42704",
  "messageParameters" : {
    "schemaName" : "`spark_catalog`.`schema 1`"
  }
}


-- !query
SELECT row_number() OVER IDENTIFIER('x.win') FROM VALUES(1) AS T(c1) WINDOW win AS (ORDER BY c1)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "MISSING_WINDOW_SPECIFICATION",
  "sqlState" : "42P20",
  "messageParameters" : {
    "docroot" : "https://spark.apache.org/docs/latest",
    "windowName" : "IDENTIFIER('x.win')"
  }
}


-- !query
SELECT T1.c1 FROM VALUES(1) AS T1(c1) JOIN VALUES(1) AS T2(c1) USING (IDENTIFIER('c1'))
-- !query analysis
Project [c1#x]
+- Project [c1#x]
   +- Join Inner, (c1#x = c1#x)
      :- SubqueryAlias T1
      :  +- LocalRelation [c1#x]
      +- SubqueryAlias T2
         +- LocalRelation [c1#x]


-- !query
SELECT IDENTIFIER('t').c1 FROM VALUES(1) AS T(c1)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "UNRESOLVED_COLUMN.WITH_SUGGESTION",
  "sqlState" : "42703",
  "messageParameters" : {
    "objectName" : "`t`",
    "proposal" : "`c1`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 8,
    "stopIndex" : 22,
    "fragment" : "IDENTIFIER('t')"
  } ]
}


-- !query
SELECT map('a', 1).IDENTIFIER('a') FROM VALUES(1) AS T(c1)
-- !query analysis
Project [map(a, 1)[IDENTIFIER('a')] AS map(a, 1)[IDENTIFIER('a')]#x]
+- SubqueryAlias T
   +- LocalRelation [c1#x]


-- !query
SELECT named_struct('a', 1).IDENTIFIER('a') FROM VALUES(1) AS T(c1)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "FIELD_NOT_FOUND",
  "sqlState" : "42704",
  "messageParameters" : {
    "fieldName" : "`IDENTIFIER('a')`",
    "fields" : "`a`"
  }
}


-- !query
SELECT * FROM s.IDENTIFIER('tab')
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "TABLE_OR_VIEW_NOT_FOUND",
  "sqlState" : "42P01",
  "messageParameters" : {
    "relationName" : "`s`.`tab`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 33,
    "fragment" : "s.IDENTIFIER('tab')"
  } ]
}


-- !query
SELECT * FROM IDENTIFIER('s').IDENTIFIER('tab')
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "TABLE_OR_VIEW_NOT_FOUND",
  "sqlState" : "42P01",
  "messageParameters" : {
    "relationName" : "`s`.`tab`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 47,
    "fragment" : "IDENTIFIER('s').IDENTIFIER('tab')"
  } ]
}


-- !query
SELECT * FROM IDENTIFIER('s').tab
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "TABLE_OR_VIEW_NOT_FOUND",
  "sqlState" : "42P01",
  "messageParameters" : {
    "relationName" : "`s`.`tab`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 15,
    "stopIndex" : 33,
    "fragment" : "IDENTIFIER('s').tab"
  } ]
}


-- !query
SELECT row_number() OVER IDENTIFIER('win') FROM VALUES(1) AS T(c1) WINDOW win AS (ORDER BY c1)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "MISSING_WINDOW_SPECIFICATION",
  "sqlState" : "42P20",
  "messageParameters" : {
    "docroot" : "https://spark.apache.org/docs/latest",
    "windowName" : "IDENTIFIER('win')"
  }
}


-- !query
SELECT row_number() OVER win FROM VALUES(1) AS T(c1) WINDOW IDENTIFIER('win') AS (ORDER BY c1)
-- !query analysis
org.apache.spark.sql.AnalysisException
{
  "errorClass" : "MISSING_WINDOW_SPECIFICATION",
  "sqlState" : "42P20",
  "messageParameters" : {
    "docroot" : "https://spark.apache.org/docs/latest",
    "windowName" : "win"
  }
}


-- !query
WITH identifier('v')(identifier('c1')) AS (VALUES(1)) (SELECT c1 FROM v)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "TABLE_OR_VIEW_NOT_FOUND",
  "sqlState" : "42P01",
  "messageParameters" : {
    "relationName" : "`v`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 71,
    "stopIndex" : 71,
    "fragment" : "v"
  } ]
}


-- !query
INSERT INTO tab(IDENTIFIER('c1')) VALUES(1)
-- !query analysis
org.apache.spark.sql.catalyst.ExtendedAnalysisException
{
  "errorClass" : "TABLE_OR_VIEW_NOT_FOUND",
  "sqlState" : "42P01",
  "messageParameters" : {
    "relationName" : "`tab`"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 13,
    "stopIndex" : 15,
    "fragment" : "tab"
  } ]
}


-- !query
CREATE OR REPLACE VIEW v(IDENTIFIER('c1')) AS VALUES(1)
-- !query analysis
CreateViewCommand `spark_catalog`.`default`.`v`, [(IDENTIFIER('c1'),None)], VALUES(1), false, true, PersistedView, COMPENSATION, true
   +- LocalRelation [col1#x]


-- !query
CREATE TABLE tab(IDENTIFIER('c1') INT) USING CSV
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`tab`, false


-- !query
CREATE TABLE IDENTIFIER('id_lite_coldef_ok')(IDENTIFIER('c1') INT) USING CSV
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`id_lite_coldef_ok`, false


-- !query
DROP TABLE IDENTIFIER('id_lite_coldef_ok')
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.id_lite_coldef_ok


-- !query
CREATE TABLE test_qualified_col_error(IDENTIFIER('col1.col2') INT) USING CSV
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "IDENTIFIER_TOO_MANY_NAME_PARTS",
  "sqlState" : "42601",
  "messageParameters" : {
    "identifier" : "col1.col2",
    "limit" : "1"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 39,
    "stopIndex" : 65,
    "fragment" : "IDENTIFIER('col1.col2') INT"
  } ]
}


-- !query
CREATE TABLE test_qualified_col_error2(id INT, IDENTIFIER('schema.table') STRING) USING CSV
-- !query analysis
org.apache.spark.sql.catalyst.parser.ParseException
{
  "errorClass" : "IDENTIFIER_TOO_MANY_NAME_PARTS",
  "sqlState" : "42601",
  "messageParameters" : {
    "identifier" : "schema.table",
    "limit" : "1"
  },
  "queryContext" : [ {
    "objectType" : "",
    "objectName" : "",
    "startIndex" : 48,
    "stopIndex" : 80,
    "fragment" : "IDENTIFIER('schema.table') STRING"
  } ]
}


-- !query
CREATE TABLE test_col_with_dot(IDENTIFIER('`col.with.dot`') INT) USING CSV
-- !query analysis
CreateDataSourceTableCommand `spark_catalog`.`default`.`test_col_with_dot`, false


-- !query
DROP TABLE test_col_with_dot
-- !query analysis
DropTable false, false
+- ResolvedIdentifier V2SessionCatalog(spark_catalog), default.test_col_with_dot


-- !query
SELECT 1 AS IDENTIFIER('col1')
-- !query analysis
Project [1 AS IDENTIFIER('col1')#x]
+- OneRowRelation


-- !query
SELECT 'hello' AS IDENTIFIER('my_column')
-- !query analysis
Project [hello AS IDENTIFIER('my_column')#x]
+- OneRowRelation


-- !query
SELECT * FROM VALUES (1, 2) AS IDENTIFIER('my_table')(IDENTIFIER('c1'), IDENTIFIER('c2'))
-- !query analysis
Project [c1#x, c2#x]
+- SubqueryAlias `IDENTIFIER('my_table')`
   +- LocalRelation [c1#x, c2#x]


-- !query
SELECT * FROM VALUES (10, 20) AS IDENTIFIER('t')(IDENTIFIER('col_a'), IDENTIFIER('col_b'))
-- !query analysis
Project [col_a#x, col_b#x]
+- SubqueryAlias `IDENTIFIER('t')`
   +- LocalRelation [col_a#x, col_b#x]


-- !query
SELECT * FROM VALUES (1, 2) AS IDENTIFIER('schema.table')(c1, c2)
-- !query analysis
Project [c1#x, c2#x]
+- SubqueryAlias `IDENTIFIER('schema.table')`
   +- LocalRelation [c1#x, c2#x]


-- !query
SELECT 1 AS IDENTIFIER('col1.col2')
-- !query analysis
Project [1 AS IDENTIFIER('col1.col2')#x]
+- OneRowRelation
